---
layout: post
title: Monads for the inquisitive
---

### Motivational introduction

_Outlines what use do we have from monads in computer science. The first three paragraphs are essential, the rest can be omitted if proves troublesome._

Functional languages are founded on the idea of function composition; having $f: A \longrightarrow B$ and $g: B \longrightarrow C$ we can create a composition $g \circ f: A \longrightarrow C$. Suppose now those functions have some additional, common flavour e.g. they may not return a result (like $f(x)=\frac{1}{x}$ for $x=0$) or return 0 or more results (like for a function computing possible chess positions from a given position after a single move).

In a strongly typed language we'd represent this additional behaviour by
a different type of the return value. Let's stick to the example of
possibly not returning a value. We'd introduce a type constructor `Maybe`, $M$ for short, and our functions would have signatures $f:A \longrightarrow MB$ and $g:B \longrightarrow MC$.

We still do want to compose them; the additional behaviour should not hinder us, they both have it. But now the types are incompatible. We want
to apply $g$ after $f$ but $g$ accepts type $B$, not $MB$. We are stuck. Monads are a device to break this impasse, let's keep this in mind for the rest of the section.

Taking a step back, another thing besides composition of functions that we would can employ is the identity function $id: A \longrightarrow A$, a trivial computation for any type $A$. Now, it turns out that there is a mathematical structure built on top of these two properties called a category.

It belongs to category theory, a mathematical area that tries to describe things in the most general way possible. Thus the definition of a category is very simple:
> A **category** $\mathcal{C}$ is a collection of:
> 1. **objects** $A, B, \ldots$
> 2. **morphisms** or **arrows** (which you could think of as maps)
>
> where the morphisms have the objects of $\mathcal{C}$ as domain and codomain, e.g. $f:A \longrightarrow B$. The morphisms of corresponding domains and codomains have to be composable in the way you'd expect and each object has its **identity morphism** $id_A:A \longrightarrow A$ that acts trivially in the composition.

For more details refer to chapter 1 in [1].

Now, further into category theory, there is a notion of a **monad**. Regardless of what is its mathematical definition (5.1 in [1]), a monad $M$ on a category $\mathcal{C}$ gives us another category, the **Kleisli category** $\mathcal{C}_\mathbb{T}$, whose morphisms are morphisms in $\mathcal{C}$ of the form $f:A\longrightarrow MB$. The fact that it's a category means we can compose those morphisms. This is what we wanted all along and this is what monads give us.

### What is a monad?

_Presents two definitions of a monad in computer science and discusses them briefly. The discussion might be initially useful, but if it gets too involved the reader should go on to the examples section._

Much in the spirit of the above reasoning, the definition of a monad in
computer science as given by [2] is as follows:
> A **monad** is a map $M$:
> 1. sending every type $X$ of a given programming language to a new type
$MX$.
> 2. equipped with a rule for composing two functions of the form
$f: X \longrightarrow MY$ and
$g: Y \longrightarrow MZ$ to $gf: X\longrightarrow MZ$.
The functions are called **Kleisli functions** and the composition the **Kleisli composition**.
> 3. together with a map $\mathrm{pure}_X: X\longrightarrow MX$
for every type $X$ that acts trivially in the composition.
The Kleisli composition is required to be [associative](https://en.wikipedia.org/wiki/Associative_property).

A definition offered by Haskell at [3] is more down to earth:
> A **monad** is:
> 1. a type constructor $m$
> 2. a function building values of type $m$, $a \longrightarrow ma$ for any type $a$ called **return**
> 3. a combinator function called **bind** of the form $(ma, a \longrightarrow mb)\longrightarrow mb$ for any type $a, b$. Typically denoted $ma\ {\scriptstyle \gg=}\ a \longrightarrow mb$.

The definition above has to also satisfy the laws equivalent to the third point in the previous definition. Unless otherwise stated, we will be referring to the first definition.

A natural question to ask would be about the meaning of the $\mathrm{pure}$ function (and its counterpart from the second definition -- bind). If all we want is a composition and this is guaranteed by the second point in the definition, why bother with the third?

$\mathrm{pure}$ and bind are in some sense trivial computations; ones such that composing them with others does not make any difference. The existence of such simple computations ensures somehow that $M$ is not too weird. There is an "obvious", "natural" way to get from a value in $X$ to a value in $MX$. Mathematically speaking this means that the computations of the form $A\longrightarrow MB$ have the structure of a category, the aforementioned Kleisli category. Why do we want it to be a cetegory? We want to exclude from the monad definition some pathological cases, those overly weird ones which would be too irregular to be of practical use anyway.

For now we will leave the following as an open question: what additional structures could we get by dropping 3? One should be interested in finding both the most pathological examples to see what we are saving ourselves from, as well as the least obscene ones to see on the other hand what we could potentially be missing.

Another question we might want to ask concerns the relation between the two definitions. Are they equivalent? First points are clearly the same, $\mathrm{pure}$ for a given type is the same as bind and the laws associated with 3. are already stated to be equivalent. The only question is whether having Kleisli composition is the same as having the bind combinator.

Indeed if we have a bind function then we get Kleisli composition by defining $gf(x) := f(x)\ {\scriptstyle \gg=}\ g$. Before looking into whether Kleisli composition gives us bind let's look at the Haskell definition more closely. If things get a little too involved the reader may well proceed to the end of this section.

The three points of the second definition together with its laws ensure that the return function is injective for any type unless $m$ is trivial. In both cases $m$ is a functor and $(m, \mathrm{return}, \left(\mathrm{return}_m)^{-1}\right)$ is a monad in category theory sense. If $\mathrm{pure}_X$ is injective for any type $X$ then indeed we get a bind but in general it looks as though it need not be.

Again, we leave these as open questions: can we construct a monad in the first sense without $\mathrm{pure}_X$ being injective? Can $M$ be non-monadic in category theory sense? Can it be non-functorial?

So in general the first definition seems broader. However, in all practical and useful cases in which computer science is actually interested the two definitions coincide and one should forget the potential discrepancy.


### Examples of monads in programming languages

_Three examples: one from Java, one language-agnostic and one from Haskell. The last one is more involved._

#### Optionals in Java

#### A list constructor

A list constructor $L$ can also be given a structure of a monad. The additional behaviour of functions in this case is them returning 0 or more results, like the functions computing chess positions mentioned in the first section. We want compose two such functions $f: \mathrm{ChPos}\longrightarrow L\mathrm{ChPos}$ to get a function computing possible chess possitions after two moves. How would we do that? $f(x)$ is a list of chess positions; we'd apply another $f$ to every such position getting a list of lists of positions. Then we'd flatten this list to a simple list of positions.

The chess positions is just an example, but it's clear that the same would apply to all functions returning a list of elements. The bind is the operator applying a function to every element of a list and flattening the result. The return for an element returns a list containing this single element. These two together with $L$ form a monad.

#### Random numbers in Haskell

Consider generating random numbers in a purely functional language like Haskell. Purely functional means that any functions always returns the same result for the same input. Therefore function like `rand` returning a random number from $(0, 1)$ are not allowed to exist.

How functions like `rand` work in "traditional", non-functional languages? They sample some data like mouse movements or time in nanoseconds from the system and produce a value based on these. Such implicit arguments are not allowed in functional languages, so what one has to do here is to pass this data explicitly to a function, in the form of some object.

So, in languages like Haskell functions returning random values take a random number generator object, rng, and return a value together with another rng (in case we want to generate more numbers in the future. We can't reuse the rng that we passed initially -- as it's explained above it would return the same value!). Their signature is $r\longrightarrow(v, r)$.

A function of this form is called a **stateful computation**, sc, because what we are doing is passing some state to the function based on which it does some stuff and returns a value together with a new state, in our case some new rng. The type of an sc is defined to be the type of the return value.

Using these, to write a function returning three random numbers we'd need to pass the rngs to three function calls.

`(firstNumber,  newGen)   = rand(gen)`<br />
`(secondNumber, newGen')  = rand(newGen)`<br />
`(thirdNumber,  newGen'') = rand(newGen')`

If you think this is ludicrous we totally agree.

But observe what we are effectively doing is composing functions returning scs of a given type. The type constructor mapping a type to stateful computations of that type turns out to have a monadic structure. Observe that $ma$ is a functional type i.e. it's values are functions, don't get lost on this.

What would be a trivial sc for a value `x`? One that would yield `x` for all states `s`.

$\mathrm{return}\ x = s \longmapsto (x,s)$

For the combinator suppose we have an sc $c: s \longmapsto (a, s)$ of some type and a function $f: a \longrightarrow (s \longrightarrow (x, s))$ from that type to scs of possibly some other type. E.g. $f(a)$ could return sc returning two values, the first of which is always $a$.

$c\ {\scriptstyle >==}\  f$ is an sc, say $c'$. Intuitively what we want it to do on a state $s$ is to do whatever $c$ did and then do whatever sc returned by $f$ does. To be precise the definition of $c'(s)$ is:

`(a, newState) = c(s)`<br />
`return f(a)(newState)`

(here and below `return` is the operation of returning a value from a function, not `return` defined above. Sorry for the overloaded notation, it's not our fault!). Now the function generating three random numbers with a bit of Haskell's syntactic sugar can look like:

`do`<br />
&nbsp;&nbsp;&nbsp;&nbsp;`a <- rand`<br />
&nbsp;&nbsp;&nbsp;&nbsp;`b <- rand`<br />
&nbsp;&nbsp;&nbsp;&nbsp;`c <- rand`<br />
&nbsp;&nbsp;&nbsp;&nbsp;`return (a,b,c)`

Thus, what monads did to us is carrying out the pesky state passing implicitly, out of our sight, making the code look much more like what we would expect in a "traditional" language. For a more careful account of the topic you could refer to [6].

### Background

The term **monad**, from Greek *monas*, "unit", was first used by Giordano Bruno.
He claimed the world is *homogeneous*, *one*; nevertheless, it consists of a multitude of self-contained units. Like a point, *mathematical minimum*, is a basic component of space, and an atom, *physical minimum*, of that of matter - monad is a *metaphysical minimum*. Their shapes and forms are unique (which followed naturally from God being the ultimate perfect creator of infinite inventiveness).

It was then picked up by Gottfried Wilhelm Leibniz, one of the most interesting philosophers of the 17th century. While he's mostly known for the fundamental contributions to differential and integral calculus, his metaphysical system was equally groundbreaking in its cohesiveness and generality.

Three hundred years ago the most enlightened minds of Europe were debating the problem of *substance*. It may seem like a silly problem now (or, to be polite, a poorly defined one) but surprisingly, these old ideas influence our language and way of thinking even today.

Rene Descartes rejected the then dominant school of thought, platonism, arguing that the *corporeal* substance consists of only matter itself, that there are no platonic forms. However, he went only so far in reducing the metaphysical overhead of previous generations - inspired by then state-of-the-art automatons on the streets of Paris, he introduced the distinction between body and mind. And that dualism lives on, three centuries later.

Descartes struggled with explaining how exactly these two substances influence each other. His successors suggested various solutions, most of which seem odd nowadays:
1. They don't influence each other and they don't have to because God does that for them
2. They may seem like two substances but in reality they are just two sides of the same coin, so to speak, namely God
3. There is no dualism, and only matter exists (no God here)

But perhaps the most original solution was Leibniz's. He claimed there is an infinite number of substances but they are of the same kind. In this respect his views were similar to materialists; but looking closer we discover significant differences.

Leibniz believed that every phenomenon is unique, that everything differs from everything else, that there are no two identical leaves or drops of water. There are no two objects that differ only by their positions in space. However, things are *similar*. Phenomena are continuous series (notice connection to differentials). Straight line is just a special case of a curve, equation - of inequality. Consciousness is a spectrum, a lie is the smallest truth, evil the smallest good. This was Leibniz's *continuity law* (lex continui). It allowed him to connect seemingly disjoint phenomena using a coherent framework. An interesting application of this theory was in psychology, where he introduced unconscious states of mind long before Freud (claiming that these were only *weak* forms of consciousness).

This law, along with his *metaphysical pluralism* was perhaps the most revolutionary thesis of Leibniz's philosophy. And here, finally, we come to monads - this is how he called these simple individual *units of being*.
Unfortunately for computer science-oriented readers, they have little in common with Haskell. Perhaps the most significant is that they *don't have windows*, as Leibniz said, each of them being a closed universe that acts upon its environment but cannot be influenced from outside.


### Bibliography

1. Peter Johnstone, Category Theory, 2015, http://pi.math.cornell.edu/~dmehrle/notes/partiii/cattheory_partiii_notes.pdf
2. Monad (in computer science), https://ncatlab.org/nlab/show/monad+%28in+computer+science%29
3. All About Monads, https://wiki.haskell.org/All_About_Monads
4. The List Monad, https://www.schoolofhaskell.com/school/starting-with-haskell/basics-of-haskell/13-the-list-monad
5. Why Do We Need Monads?, https://stackoverflow.com/questions/28139259/why-do-we-need-monads
6. For a Few Monads More - Learn you a Haskell for Great Good!, http://learnyouahaskell.com/for-a-few-monads-more
