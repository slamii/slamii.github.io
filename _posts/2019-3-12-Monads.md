---
layout: post
title: Monads for the inquisitive
---

### Motivational introduction

Functional languages are founded on the idea of function composition; having $f: A \longrightarrow B$ and $g: B \longrightarrow C$ we can create a composition$g \circ f: A \longrightarrow C$. Suppose now those functions have some additional, common flavour e.g. they may not return a result (like $f(x)=\frac{1}{x}$ for $x=0$) or return 0 or more results (like for a function computing possible chess positions from a given position after a single move).

In a strongly typed language we'd represent this additional behaviour by
a different type of the return value. Let's stick to the example of 
possibly not returning a value. We'd introduce a type constructor `Maybe`, $M$ for short, and our functions would have signatures $f:A \longrightarrow MB$ and $g:B \longrightarrow MC$.

We still do want to compose them; the additional behaviour should not hinder us, they both have it. But now the types are incompatible. We want 
to apply $g$ after $f$ but $g$ accepts type $B$, not $MB$. We are stuck. Monads are a device to break this impasse, let's keep this in mind for the rest of the section.

Taking a step back, another thing besides composition of functions that we would can employ is the identity function $id: A \longrightarrow A$, a trivial computation for any type $A$. Now, it turns out that there is a mathematical structure built on top of these two properties called a category.

It belongs to Category Theory, a mathematical area that tries to describe things in the most general way possible. Thus the definition of a category is very simple: 
> A **category** $\mathcal{C}$ is a collection of: 
> 1. **objects** $A, B, \ldots$
> 2. **morphisms** or **arrows** (which you could think of as maps)
>
> where the morphisms have the objects of $\mathcal{C}$ as domain and codomain, e.g. $f:A \longrightarrow B$. The morphisms of corresponding domains and codomains have to be composable in the way you'd expect and each object has its **identity morphism** $id_A:A \longrightarrow A$ that acts trivially in the composition. 

For more details refer to [Johnstone notes].

Now, further into Category Theory, there is a notion of a **monad**. Regardless of what is its mathematical definition (refer to 5.1 in [Johnstone notes]), a monad $M$ on a category $\mathcal{C}$ gives us another category, the **Kleisli category** $\mathcal{C}_\mathbb{T}$, whose morphisms are morphisms in $\mathcal{C}$ of the form $f:A\longrightarrow MB$. The fact that it's a category means we can compose those morphisms. This is what we wanted all along and this is what monads give us.

### What is a monad?

Much in the spirit of the above reasoning, the definition of a monad in
computer science as given by [nLab] is as follows:
> A **monad** is a map $M$:
> 1. sending every type $X$ of a given programming language to a new type
$MX$.
> 2. equipped with a rule for composing two functions of the form
$f: X \longrightarrow MY$ and 
$g: Y \longrightarrow MZ$ to $gf: X\longrightarrow MZ$.
The functions are called **Kleisli functions** and the composition the **Kleisli composition**.
> 3. together with a map $\mathrm{pure}_X: X\longrightarrow MX$
for every type $X$ that acts trivially in the composition.
The Kleisli composition is required to be [associative](https://en.wikipedia.org/wiki/Associative_property).

A definition offered by [Haskell All About Monads] is more down to earth:
> A **monad** is:
> 1. a type constructor $m$
> 2. a function building values of type $m$, $a \longrightarrow ma$ for any type $a$ called **return**
> 3. a combinator function called **bind** of the form $(ma, a \longrightarrow mb)\longrightarrow mb$ for any type $a, b$. Typically denoted $ma {\scriptstyle >>=} a \longrightarrow mb$.

The definition above has to also satisfy the laws equivalent to the third point in the nLab's definition. Unless otherwise stated, we will be referring to the first definition.

A natural question to ask would be about the meaning of the $\mathrm{pure}$ function (and its counterpart from the second definition -- bind). If all we want is a composition and this is guaranteed by the second point in the definition, why bother with the third?

$\mathrm{pure}$ and bind are in some sense trivial computations; ones such that composing them with others does not make any difference. The existence of such simple computations ensures somehow that $M$ is not too weird. There is an "obvious", "natural" way to get from a value in $X$ to a value in $MX$. Mathematically speaking this means that the computations of the form $A\longrightarrow MB$ have the structure of a category, the aforementioned Kleisli category. Why do we want it to be a cetegory? We want to exclude from the definition some pathological cases, those overly weird ones which would be too irregular to be of practical use anyway.

For now we will leave the following as an open question: what additional structures could we get by dropping 3? One should be interested in finding both the most pathological examples to see what we are saving ourselves from, as well as the least obscene ones to see on the other hand what we could potentially be missing.

Another question we might want to ask concerns the relation between the two definitions. Are they equivalent? First points are clearly the same, $\mathrm{pure}$ for a given type is the same as bind and the laws associated with 3. are already stated to be equivalent. The only question is whether having Kleisli composition is the same as having the bind combinator.

Indeed if we have a bind function then we get Kleisli composition by defining $gf(x) := f(x) {\scriptstyle >>=} g$. Before looking into whether Kleisli composition gives us bind let's look at the Haskell definition more closely. Things will get a little more involved for a moment; if the reader gets lost, she may well proceed to the end of this section.

The three points of Haskell definition together with its laws ensure that the return function is injective for any type unless $m$ is trivial. In both cases $m$ is a functor and $(m, \mathrm{return}, \left(\mathrm{return}_m)^{-1}\right)$ is a monad in Category Theory sense. If $\mathrm{pure}_X$ is injective for any type $X$ then indeed we get a bind but in general it looks as though it need not be.

Again, we leave these as open questions: can we construct a monad in the nLab sense without $\mathrm{pure}_X$ being injective? Can $M$ be non-monadic in Category Theory sense? Can it be non-functorial?

So in general the nLab's definition seems broader. However, in all practical and useful cases in which Computer Science is actually interested the two definitions coincide and one should forget the potential discrepancy.


### Examples of monads in programming languages

#### Optionals in Java

#### A list constructor

A list constructor $L$ can also be given a structure of a monad. The additional behaviour of functions in this case is them returning 0 or more results, like the functions computing chess positions mentioned in the first section. We want compose two such functions $f: \mathrm{ChPos}\longrightarrow L\mathrm{ChPos}$ to get a function computing possible chess possitions after two moves. How would we do that? $f(x)$ is a list of chess positions, we'd apply another $f$ to every such position getting a list of lists of positions. Then we'd flatten this list to a simple list of positions.

The chess positions is just an example, but it's clear that the same would apply to any functions returning a list of elements. The bind is the operator applying a function to every element of a list and flattening the result. The return for an element returns a list containing this single element. These two together with $L$ form a monad.

#### Random numbers in Haskell

Consider generating random numbers in a purely functional language, like Haskell. Purely functional means that for same inputs functions always return the same results. Therefore function like `rand` returning a random number from $(0, 1)$ are allowed to exist.

How functions like `rand` work in "traditional", non-functional languages? They sample some data from the system like mouse movements or time in nanoseconds and produce a value based on these. Such implicit "arguments" are not allowed in functional languages, so what one has to do here is to pass this data explicitly to a function, in the form of some object.

So, in languages like Haskell functions returning random values take a random number generator object (rng) and return a value together with another rng (in case we want to generate more numbers in the future. We can't reuse the rng that we passed initially - as it's explained above it would return the same value!). Their signature is $r\longrightarrow(v, r)$.

A function of this type is called a **stateful computation**, sc, because what we are doing is passing some state to the function, based on which it does some stuff and returns value together with a new state, in our case some new rng. The type of an sc is the type of the return value. To write a function returning three random numbers we'd need to pass the rngs to three function calls.

`(firstNumber, newGen) = rand(gen)`
`(secondNumber, newGen') = rand(newGen)`
`(thirdNumber, newGen'') = rand(newGen')`

If you think this is ludicrous we totally agree.

But observe what we are effectively doing is composing functions returning scs, of a given type. The type constructor

$$ma = \left\{\mathrm{stateful computations of type} a\right\}

turns out to have a monadic structure. Observe that $ma$ is a functional type i.e. it's values are functions, don't get lost on this.

What would be a trivial sc for a value `x`? One that would return `x` for all states `s`.

`return x = s -> (x,s)`

What is the combinator? Suppose we have an sc of type `c: s -> (a, s)` and a function `f: a -> (s -> (x, s))` from that type to scs of possibly some other type. For example `f(a)` could return sc returning two values, the first of which is always `a`.

`c >== f` is an sc, say `c'`. Intuitively what we want it to do on a state `s` is to do whatever `c` did and then do whatever cs returned by `f` does. To be precise the definition of `c'(s)` is:

`(a, newState) = c(s)`
`return f(a)(newState)`

(here and below `return` is the operation of returning a value from a function, not `return` defined above. Sorry for the overloaded notation, it's not our fault!). Now the function generating three random numbers, with a bit of Haskell's syntactic sugar can look like:

`do`
`  a <- rand`
`  b <- rand`
`  c <- rand`
`  return (a,b,c)`

Thus, what monads did to us is carrying out the pesky state passing implicitly, out of our sight, making the code look much more like what we would expect in a "traditional" language. For a more careful account of the topic you could refer to [For a few monads more, Input and Output, Learn Your Haskell for Great Good].

### Background

The term **monad**, from Greek *monas*, "unit", was first used by Giordano Bruno.
He claimed the world is *homogeneous*, *one*. Nevertheless, it consists of a multitude
of self-contained units. Like a point, *mathematical minimum*, is a basic component of space,
and an atom, *physical minimum*, of that of matter - monad is a *metaphysical minimum*. Their
shapes and formes are unique (which followed naturally from God being the ultimate perfect
creator).

